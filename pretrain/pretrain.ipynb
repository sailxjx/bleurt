{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "25baeb81-deb1-49b2-b9c5-674b00a682dd"
   },
   "source": [
    "# Build synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "uuid": "ea0928c9-c4ae-4434-86de-6462031c41e1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"./webtext2019zh/web_text_zh_train_sample.json\", \"r\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "content = map(json.loads, content)\n",
    "content = pd.DataFrame(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "uuid": "cebc7076-f3d9-4b12-98e2-3347b734979b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 ms, sys: 1e+03 ns, total: 2.04 ms\n",
      "Wall time: 2.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "\n",
    "def cut_sentences(text, min_len=3):\n",
    "    \"\"\"\n",
    "    Cut sentences by their length and punctuation, remove all spaces.\n",
    "    \"\"\"\n",
    "    text = text.replace(\" \", \"\")\n",
    "    corpus = re.split(\"[\\,\\.\\?，。？\\n]\", text)\n",
    "    corpus = list(filter(lambda x: len(x) >= min_len, corpus))\n",
    "    return corpus\n",
    "\n",
    "text = \"\\n\".join(content.content.values)\n",
    "sentences = cut_sentences(text)\n",
    "df = pd.DataFrame({\n",
    "    \"seq\": sentences\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "a2939161-05ca-427a-9025-31ee4174ef5e"
   },
   "source": [
    "# Mask filling\n",
    "Inserting masks at random positions in the Wikipedia sentences, and fill them with the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "uuid": "cd2460d1-94a6-43c6-8f26-bf14cb6d0879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 429 ms, sys: 5.01 ms, total: 434 ms\n",
      "Wall time: 432 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>masked</th>\n",
       "      <th>masked_rate</th>\n",
       "      <th>filled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我对你仍有爱意</td>\n",
       "      <td>我对你仍有爱[MASK]</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>我对你仍有感心</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我对自己无能为力</td>\n",
       "      <td>我[MASK][MASK][MASK][MASK]能为力</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>我对自己无力力力</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...</td>\n",
       "      <td>讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...</td>\n",
       "      <td>0.198953</td>\n",
       "      <td>讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我看了上帝之眼感觉还不错</td>\n",
       "      <td>我看了[MASK][MASK]之眼感觉还不错</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>我看了上帝之眼感觉的感。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>不喜欢那些讲技术的摄影书</td>\n",
       "      <td>不喜欢那些讲技术的摄[MASK][MASK]</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>不、、、、、、、、、、、</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 seq  \\\n",
       "0                                            我对你仍有爱意   \n",
       "1                                           我对自己无能为力   \n",
       "2  讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...   \n",
       "3                                       我看了上帝之眼感觉还不错   \n",
       "4                                       不喜欢那些讲技术的摄影书   \n",
       "\n",
       "                                              masked  masked_rate  \\\n",
       "0                                       我对你仍有爱[MASK]     0.142857   \n",
       "1                       我[MASK][MASK][MASK][MASK]能为力     0.500000   \n",
       "2  讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...     0.198953   \n",
       "3                             我看了[MASK][MASK]之眼感觉还不错     0.166667   \n",
       "4                             不喜欢那些讲技术的摄[MASK][MASK]     0.166667   \n",
       "\n",
       "                                              filled  \n",
       "0                                            我对你仍有感心  \n",
       "1                                           我对自己无力力力  \n",
       "2  讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...  \n",
       "3                                       我看了上帝之眼感觉的感。  \n",
       "4                                       不、、、、、、、、、、、  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "import math\n",
    "\n",
    "s = \"看到这一幕我简直兴奋得要上天了 更新于上一个答案几个小时后 剧透醒目 论这个世界上还有什么比妖怪夫妇联手更恐怖的事\"\n",
    "\n",
    "def mask_replacing(s):\n",
    "    \"\"\"\n",
    "    The first strategy samples random words in the sentence and it replaces them with masks(one for each token).\n",
    "    \"\"\"\n",
    "    seq = list(s)\n",
    "    seq_len = len(s)\n",
    "    # Sample from 1 to 50% chars of the sequence\n",
    "    k = random.randint(1, math.floor(seq_len * 0.5))\n",
    "    token_idx = random.choices(range(seq_len), k = k)\n",
    "    for i in token_idx:\n",
    "        seq[i] = \"[MASK]\"\n",
    "    return \"\".join(seq)\n",
    "\n",
    "def mask_replacing2(s):\n",
    "    \"\"\"\n",
    "    The second strategy cre-ates contiguous sequences: \n",
    "    it samples a start po-sition s, a length l (uniformly distributed), \n",
    "    and it masks all the tokens spanned by words betweenpositions s and s + l.\n",
    "    \"\"\"\n",
    "    seq_len = len(s)\n",
    "    start = random.randint(1, seq_len-1)\n",
    "    # At least 10% of words\n",
    "    min_length = min(math.floor(seq_len * 0.1), seq_len - start)\n",
    "    min_length = max(min_length, 1)\n",
    "    # At most 50% of words\n",
    "    max_length = min(math.floor(seq_len * 0.5), seq_len - start)\n",
    "    max_length = max(min_length, max_length)\n",
    "    length = random.choice(range(min_length, max_length+1))\n",
    "    \n",
    "    s = s[:start] + \"[MASK]\" * length + s[(start+length):]\n",
    "    return pd.Series([s, length / seq_len], index=[\"masked\", \"masked_rate\"])\n",
    "\n",
    "masked_seqs = pd.Series(sentences).apply(mask_replacing2)\n",
    "df[\"masked\"] = masked_seqs[\"masked\"]\n",
    "df[\"masked_rate\"] = masked_seqs[\"masked_rate\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "30d70ee0-f106-40bd-9333-cdf85bceb28f"
   },
   "outputs": [],
   "source": [
    "# Fill in the masked fields\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/admin/workspace/model/transformers/bert-base-multilingual-cased')\n",
    "model = AutoModelForMaskedLM.from_pretrained('/home/admin/workspace/model/transformers/bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "a40af827-1da8-4c57-9d8f-6ce9928e42aa"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "def mask_replacing(text):\n",
    "#     text = \"我对自[MASK][MASK]能为力\"\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    [predictions] = model(**encoded_input)\n",
    "\n",
    "    predicted_index = torch.argmax(predictions[0], dim=1).tolist()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\n",
    "    return \"\".join(predicted_token[1:-1])\n",
    "\n",
    "filled_seqs = df.masked.apply(mask_replacing).rename(\"filled\")\n",
    "df[\"filled\"] = filled_seqs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "uuid": "434b2808-45f4-457b-9171-28d75b6daecb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the model checkpoint at /home/admin/workspace/model/transformers/bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".我对自己功能为力力\n"
     ]
    }
   ],
   "source": [
    "# tensorflow model\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/admin/workspace/model/transformers/bert-base-multilingual-cased\")\n",
    "model = TFAutoModelWithLMHead.from_pretrained(\"/home/admin/workspace/model/transformers/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "uuid": "1e72e38d-baca-4785-9228-19f8fa99762c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我对自己功能为力\n"
     ]
    }
   ],
   "source": [
    "text = \"我对自[MASK][MASK]能为力\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "[predictions] = model(encoded_input)\n",
    "\n",
    "predicted_index = tf.argmax(predictions[0], axis=1)\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\n",
    "print(\"\".join(predicted_token[1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "c23e33ae-9699-4d22-823e-48118342527d"
   },
   "source": [
    "# Backtranslation\n",
    "Translate chinese to english, and translate back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "uuid": "8e422716-fa36-4a29-a718-ed10984a43de"
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from aliyunsdkcore.client import AcsClient\n",
    "from aliyunsdkcore.acs_exception.exceptions import ClientException\n",
    "from aliyunsdkcore.acs_exception.exceptions import ServerException\n",
    "from aliyunsdkalimt.request.v20181012.TranslateGeneralRequest import TranslateGeneralRequest\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"/home/admin/workspace/.secret\")\n",
    "\n",
    "client = AcsClient(config[\"account xjx\"][\"access_key\"], \n",
    "                   config[\"account xjx\"][\"access_secret\"], \n",
    "                   'cn-hangzhou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "uuid": "e633e4c8-4f52-4e15-ab5d-8eb5e70e2e88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 200, 300, 400, 500, 600, 700, 800, 900]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = list(range(1000))\n",
    "l[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "uuid": "18c57cfb-6936-4474-91af-603a8415a8f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'c'], dtype='<U1')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a']\n",
    "b = ['b']\n",
    "c = ['c']\n",
    "np.concatenate([a, b, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "uuid": "5e4bd9d6-36b5-4bc7-ba70-d08a1c42ce34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 2 ms, total: 170 ms\n",
      "Wall time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "class BackTranslation:\n",
    "    def __init__(self):\n",
    "        self.bulk_size = 4800\n",
    "    \n",
    "    def back_translation(self, corpus):\n",
    "        translated = self._bulk_translate(corpus, from_lang = \"zh\", to_lang = \"en\")\n",
    "        back_translated = self._bulk_translate(translated, from_lang = \"en\", to_lang = \"zh\")\n",
    "        return back_translated\n",
    "    \n",
    "    def _bulk_translate(self, corpus, from_lang = \"zh\", to_lang = \"en\"):\n",
    "        translated = []\n",
    "        text = \"\"\n",
    "\n",
    "        def _do_translate(text, translated):\n",
    "            translated_text = self._translate(text.strip(), from_lang = from_lang, to_lang = to_lang)\n",
    "            translated +=  translated_text.split(\"\\n\")\n",
    "            \n",
    "        for seq in corpus:\n",
    "            if len(text + seq) >= self.bulk_size:\n",
    "                _do_translate(text, translated)\n",
    "                text = seq + \"\\n\"\n",
    "            else:\n",
    "                text += seq + \"\\n\"\n",
    "                \n",
    "        _do_translate(text, translated)\n",
    "        \n",
    "        return translated\n",
    "    \n",
    "    def _translate(self, text, from_lang = \"zh\", to_lang = \"en\"):\n",
    "        \"\"\"\n",
    "        The api of alimt has limit the maximum length of text to 5000 characters, maximum QPS to 50,\n",
    "        so we should send the request in several bulks, with less than 250000 characters in each bulk.\n",
    "        \"\"\"\n",
    "        request = TranslateGeneralRequest()\n",
    "        request.set_accept_format('json')\n",
    "\n",
    "        request.set_FormatType(\"text\")\n",
    "        request.set_SourceLanguage(from_lang)\n",
    "        request.set_TargetLanguage(to_lang)\n",
    "\n",
    "        request.set_SourceText(text)\n",
    "\n",
    "        response = client.do_action_with_exception(request)\n",
    "        response_json = json.loads(response)\n",
    "    \n",
    "        try:\n",
    "            translated = response_json[\"Data\"][\"Translated\"]\n",
    "            return translated\n",
    "        except:\n",
    "            print(response_json)\n",
    "            raise Exception(\"Response error\")\n",
    "    \n",
    "def parallelize(df, func):\n",
    "    partitions = multiprocessing.cpu_count()\n",
    "    df_splited = np.array_split(df, partitions)\n",
    "    df_splited = Parallel(\n",
    "        n_jobs=partitions\n",
    "    )(delayed(func)(df) for df in df_splited)\n",
    "    return np.concatenate(df_splited)\n",
    "\n",
    "back_translated = parallelize(df.seq.values, BackTranslation().back_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "uuid": "76720da7-431a-4fe7-9ee3-1931ced44543"
   },
   "outputs": [],
   "source": [
    "df[\"back_translated\"] = back_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "uuid": "bf006d22-b460-4140-a057-cb163c09b0e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>masked</th>\n",
       "      <th>masked_rate</th>\n",
       "      <th>filled</th>\n",
       "      <th>back_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>这样才能酿出酒精度12的葡萄酒</td>\n",
       "      <td>这样才能酿出[MASK][MASK]度12的葡萄酒</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>这样才能酿出酒印度12的葡萄酒</td>\n",
       "      <td>为了酿造酒精 12 酒</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>新疆的鲜食葡萄很出名</td>\n",
       "      <td>新疆的鲜食葡萄很出[MASK]</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>新疆的鲜食葡萄很出色</td>\n",
       "      <td>新疆葡萄很有名</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>酿酒葡萄品质也是很好的</td>\n",
       "      <td>酿酒葡萄[MASK][MASK][MASK]是很好的</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>酿酒葡萄酒酒酒是很好的</td>\n",
       "      <td>酿酒葡萄品质也很好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>中国每年生产的葡萄酒</td>\n",
       "      <td>中[MASK][MASK]年生产的葡萄酒</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>中国早年生产的葡萄酒</td>\n",
       "      <td>生产的葡萄酒，每年在中国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>有很大的比重都是用新疆的酿酒葡萄汁酿造的</td>\n",
       "      <td>有很大的比重都是用新疆的酿酒葡萄[MASK][MASK]造的</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>有很大的比重都是用新疆的酿酒葡萄酒釀造的</td>\n",
       "      <td>有很大一部分用于新疆葡萄汁酿造</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       seq                          masked  masked_rate  \\\n",
       "1011       这样才能酿出酒精度12的葡萄酒       这样才能酿出[MASK][MASK]度12的葡萄酒     0.133333   \n",
       "1012            新疆的鲜食葡萄很出名                 新疆的鲜食葡萄很出[MASK]     0.100000   \n",
       "1013           酿酒葡萄品质也是很好的      酿酒葡萄[MASK][MASK][MASK]是很好的     0.272727   \n",
       "1014            中国每年生产的葡萄酒            中[MASK][MASK]年生产的葡萄酒     0.200000   \n",
       "1015  有很大的比重都是用新疆的酿酒葡萄汁酿造的  有很大的比重都是用新疆的酿酒葡萄[MASK][MASK]造的     0.100000   \n",
       "\n",
       "                    filled  back_translated  \n",
       "1011       这样才能酿出酒印度12的葡萄酒      为了酿造酒精 12 酒  \n",
       "1012            新疆的鲜食葡萄很出色          新疆葡萄很有名  \n",
       "1013           酿酒葡萄酒酒酒是很好的        酿酒葡萄品质也很好  \n",
       "1014            中国早年生产的葡萄酒     生产的葡萄酒，每年在中国  \n",
       "1015  有很大的比重都是用新疆的酿酒葡萄酒釀造的  有很大一部分用于新疆葡萄汁酿造  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "uuid": "237c3568-083f-4a86-aa53-b2c4d8d2a6f5"
   },
   "outputs": [],
   "source": [
    "with open(\"./homebrewed/test_pred_result_regular_trained.txt\") as f:\n",
    "    corpus = f.readlines()\n",
    "corpus = map(lambda x: corpus.split(\"[EOS]\"), corpus)\n",
    "corpus = list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "uuid": "eb41336c-70ef-4712-9e68-db474b221529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 208 ms, sys: 494 ms, total: 702 ms\n",
      "Wall time: 8.74 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['如期完成周雄辉集团KPI考核内容的改进建议。', '成功举办员工大讲堂 ~ 智能高速专题在线培训如期举行。',\n",
       "       '本周未完成和 @ 智博、 @ Liting沟通活动运营工具需求细化将推迟到下周。', ...,\n",
       "       '本周，与区投资促进局签署了投资促进机构协议，预计下周将讨论结果。', '员工培训的IVR调整计划按时顺利完成。',\n",
       "       '主要关注用于员工手册的编写 (参考上海公司)。'], dtype='<U70')"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_translated = parallelize(corpus, lambda corpus: BackTranslation().back_translation(corpus))\n",
    "back_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "uuid": "39667a32-da1a-4288-8a63-61de1f8d033d"
   },
   "outputs": [],
   "source": [
    "with open(\"./homebrewed/back_translated.txt\", \"w\") as f:\n",
    "    for (s1, s2) in zip(corpus, back_translated):\n",
    "        f.write(\"{}[SEP]{}\\n\".format(s1, s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "0b30f8e8-9bc4-40d7-8b0a-5346608fbfbe"
   },
   "source": [
    "# Word dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "8a3193c5-cd6a-4d92-b419-b81f2a626a5c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
