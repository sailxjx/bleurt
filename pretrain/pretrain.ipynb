{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "8fe91f38-189d-41be-8eea-fbfd38954269"
   },
   "source": [
    "# Build synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "uuid": "56a1b701-5bf4-40fb-9380-6a5ebe895df2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answerer_tags</th>\n",
       "      <th>content</th>\n",
       "      <th>desc</th>\n",
       "      <th>qid</th>\n",
       "      <th>star</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106030335</td>\n",
       "      <td>傲娇但不病</td>\n",
       "      <td>我对你仍有爱意，我对自己无能为力</td>\n",
       "      <td>情感</td>\n",
       "      <td>47448139</td>\n",
       "      <td>3</td>\n",
       "      <td>有哪些富有哲理又押韵的短句？（最好是情感方面）</td>\n",
       "      <td>情感</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74154645</td>\n",
       "      <td>买不起刀的游侠</td>\n",
       "      <td>讲一个同学的事儿吧 那年他小学二年级 数学课 老师讲课的时候 同学跟同桌一块儿玩儿 小孩儿贪...</td>\n",
       "      <td>可以是正面或负面</td>\n",
       "      <td>37916573</td>\n",
       "      <td>4</td>\n",
       "      <td>有哪些老师影响了你的一生？</td>\n",
       "      <td>生活</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72665930</td>\n",
       "      <td>除了我的专业什么都会一点</td>\n",
       "      <td>我看了上帝之眼感觉还不错。不喜欢那些讲技术的摄影书， 喜欢美用诗意和哲理来表现。</td>\n",
       "      <td></td>\n",
       "      <td>37403484</td>\n",
       "      <td>4</td>\n",
       "      <td>对摄影师来说，一本好的摄影书是什么样子？</td>\n",
       "      <td>摄影</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91554224</td>\n",
       "      <td>享乐一时/投身考研大军/老子要有钱</td>\n",
       "      <td>汉子已经被我收啦 现在是男票了哈哈哈哈哈 放两张聊天记录你们感受下</td>\n",
       "      <td></td>\n",
       "      <td>40985905</td>\n",
       "      <td>12</td>\n",
       "      <td>一个撩妹高手遇到一个撩汉高手是一种怎样的体验？</td>\n",
       "      <td>恋爱技巧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89481895</td>\n",
       "      <td>爱打游戏并且也学游戏的少女</td>\n",
       "      <td>看到这一幕我简直兴奋得要上天了 更新于上一个答案几个小时后 剧透醒目 论这个世界上还有什么比...</td>\n",
       "      <td>纸牌屋第四季于2016年3月4日回归&lt;img src</td>\n",
       "      <td>40991383</td>\n",
       "      <td>4</td>\n",
       "      <td>如何评价《纸牌屋》第四季？</td>\n",
       "      <td>美剧</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id      answerer_tags  \\\n",
       "0  106030335              傲娇但不病   \n",
       "1   74154645            买不起刀的游侠   \n",
       "2   72665930       除了我的专业什么都会一点   \n",
       "3   91554224  享乐一时/投身考研大军/老子要有钱   \n",
       "4   89481895      爱打游戏并且也学游戏的少女   \n",
       "\n",
       "                                             content  \\\n",
       "0                                   我对你仍有爱意，我对自己无能为力   \n",
       "1  讲一个同学的事儿吧 那年他小学二年级 数学课 老师讲课的时候 同学跟同桌一块儿玩儿 小孩儿贪...   \n",
       "2           我看了上帝之眼感觉还不错。不喜欢那些讲技术的摄影书， 喜欢美用诗意和哲理来表现。   \n",
       "3                  汉子已经被我收啦 现在是男票了哈哈哈哈哈 放两张聊天记录你们感受下   \n",
       "4  看到这一幕我简直兴奋得要上天了 更新于上一个答案几个小时后 剧透醒目 论这个世界上还有什么比...   \n",
       "\n",
       "                         desc       qid  star                    title topic  \n",
       "0                          情感  47448139     3  有哪些富有哲理又押韵的短句？（最好是情感方面）    情感  \n",
       "1                    可以是正面或负面  37916573     4            有哪些老师影响了你的一生？    生活  \n",
       "2                              37403484     4     对摄影师来说，一本好的摄影书是什么样子？    摄影  \n",
       "3                              40985905    12  一个撩妹高手遇到一个撩汉高手是一种怎样的体验？  恋爱技巧  \n",
       "4  纸牌屋第四季于2016年3月4日回归<img src  40991383     4            如何评价《纸牌屋》第四季？    美剧  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read dataset\n",
    "content = pd.read_json(\"./webtext2019zh/web_text_zh_train_sample.json\", lines=True)\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "uuid": "c8f98907-f86f-48a7-aab6-69ad0cdf4d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 ms, sys: 0 ns, total: 1.65 ms\n",
      "Wall time: 1.63 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "\n",
    "def cut_sentences(text, min_len=3, max_len=100):\n",
    "    \"\"\"\n",
    "    Cut sentences by their length and punctuation, remove all spaces.\n",
    "    \"\"\"\n",
    "    text = text.replace(\" \", \"\")\n",
    "    corpus = re.split(\"[\\,\\.\\?，。？\\n]\", text)\n",
    "    corpus = filter(lambda x: len(x) >= min_len, corpus)\n",
    "    corpus = map(lambda x: x[:max_len], corpus)\n",
    "    return list(corpus)\n",
    "\n",
    "text = \"\\n\".join(content.content.values)\n",
    "sentences = cut_sentences(text)\n",
    "df = pd.DataFrame({\n",
    "    \"seq\": sentences\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "uuid": "56b272f0-4285-46a2-859f-43dad5d1669e"
   },
   "outputs": [],
   "source": [
    "# Prepare data uploaded to ODPS\n",
    "# command: odpscmd -e \"tunnel upload ./raw_text.csv bleurt_dataset\"\n",
    "df_raw_text = df.copy()\n",
    "df_raw_text.reset_index(inplace=True)\n",
    "df_raw_text.to_csv(\"./data_generated/raw_text.csv\", index=None, header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "72adeb1b-04fd-4289-b969-433e38fdecdd"
   },
   "source": [
    "# Mask filling\n",
    "Inserting masks at random positions in the Wikipedia sentences, and fill them with the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "27a1f473-f3de-4da7-8f34-d4e29a5b54bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 394 ms, sys: 13.5 ms, total: 407 ms\n",
      "Wall time: 406 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>masked</th>\n",
       "      <th>masked_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我对你仍有爱意</td>\n",
       "      <td>[MASK]对你仍有爱意</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我对自己无能为力</td>\n",
       "      <td>我对自己无[MASK]为力</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...</td>\n",
       "      <td>讲一个[MASK]学[MASK]事[MASK][MASK]那年他小[MASK]二年级数[MA...</td>\n",
       "      <td>0.204188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我看了上帝之眼感觉还不错</td>\n",
       "      <td>我[MASK][MASK][MASK]帝之眼感[MASK][MASK]不错</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>不喜欢那些讲技术的摄影书</td>\n",
       "      <td>[MASK][MASK][MASK][MASK]些[MASK]技[MASK][MASK]摄影书</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 seq  \\\n",
       "0                                            我对你仍有爱意   \n",
       "1                                           我对自己无能为力   \n",
       "2  讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...   \n",
       "3                                       我看了上帝之眼感觉还不错   \n",
       "4                                       不喜欢那些讲技术的摄影书   \n",
       "\n",
       "                                              masked  masked_rate  \n",
       "0                                       [MASK]对你仍有爱意     0.142857  \n",
       "1                                      我对自己无[MASK]为力     0.125000  \n",
       "2  讲一个[MASK]学[MASK]事[MASK][MASK]那年他小[MASK]二年级数[MA...     0.204188  \n",
       "3              我[MASK][MASK][MASK]帝之眼感[MASK][MASK]不错     0.666667  \n",
       "4    [MASK][MASK][MASK][MASK]些[MASK]技[MASK][MASK]摄影书     0.750000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "import math\n",
    "\n",
    "s = \"看到这一幕我简直兴奋得要上天了 更新于上一个答案几个小时后 剧透醒目 论这个世界上还有什么比妖怪夫妇联手更恐怖的事\"\n",
    "\n",
    "def mask_replacing(s):\n",
    "    \"\"\"\n",
    "    The first strategy samples random words in the sentence and it replaces them with masks(one for each token).\n",
    "    \"\"\"\n",
    "    seq = list(s)\n",
    "    seq_len = len(s)\n",
    "    # Sample from 1 to 90% chars of the sequence\n",
    "    k = random.randint(1, math.floor(seq_len * 0.9))\n",
    "    token_idx = random.choices(range(seq_len), k = k)\n",
    "    for i in token_idx:\n",
    "        seq[i] = \"[MASK]\"\n",
    "    masked_rate = len(token_idx) / seq_len\n",
    "    masked = \"\".join(seq)\n",
    "    return pd.Series([masked, masked_rate], index=[\"masked\", \"masked_rate\"])\n",
    "\n",
    "def mask_replacing2(s):\n",
    "    \"\"\"\n",
    "    The second strategy cre-ates contiguous sequences: \n",
    "    it samples a start po-sition s, a length l (uniformly distributed), \n",
    "    and it masks all the tokens spanned by words betweenpositions s and s + l.\n",
    "    \"\"\"\n",
    "    seq_len = len(s)\n",
    "    start = random.randint(1, seq_len-1)\n",
    "    # At least 10% of words\n",
    "    min_length = min(math.floor(seq_len * 0.1), seq_len - start)\n",
    "    min_length = max(min_length, 1)\n",
    "    # At most 90% of words\n",
    "    max_length = min(math.floor(seq_len * 0.9), seq_len - start)\n",
    "    max_length = max(min_length, max_length)\n",
    "    length = random.choice(range(min_length, max_length+1))\n",
    "    \n",
    "    s = s[:start] + \"[MASK]\" * length + s[(start+length):]\n",
    "    return pd.Series([s, length / seq_len], index=[\"masked\", \"masked_rate\"])\n",
    "\n",
    "masked_seqs = pd.Series(sentences).apply(mask_replacing)\n",
    "df[\"masked\"] = masked_seqs[\"masked\"]\n",
    "df[\"masked_rate\"] = masked_seqs[\"masked_rate\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "1633b288-d2a0-45e7-8e35-012a39c1ae42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the model checkpoint at /home/admin/workspace/model/transformers/bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# tensorflow model\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/admin/workspace/model/transformers/bert-base-multilingual-cased\")\n",
    "model = TFAutoModelWithLMHead.from_pretrained(\"/home/admin/workspace/model/transformers/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "uuid": "1a335434-3f2d-4562-a229-b0365dc2ccce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 193)\n",
      "(16, 26)\n",
      "(16, 43)\n",
      "(16, 40)\n",
      "(16, 42)\n",
      "(16, 25)\n",
      "(4, 18)\n",
      "CPU times: user 2.8 s, sys: 507 ms, total: 3.31 s\n",
      "Wall time: 2.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def mask_filling(input_texts, batch_size=16):\n",
    "#     input_texts = [\"我对自[MASK][MASK]能为力\"]\n",
    "    if len(input_texts) < batch_size:\n",
    "        input_texts = [input_texts]\n",
    "    else:\n",
    "        # Break input texts into chunks of batch_size\n",
    "        # to avoid OOM\n",
    "        n = batch_size\n",
    "        input_texts = [input_texts[i * n:(i + 1) * n] \\\n",
    "                       for i in range((len(input_texts) + n - 1) // n )]\n",
    "    \n",
    "    filled_seqs = []\n",
    "    for chunk_texts in input_texts:\n",
    "        encoded_input = tokenizer(chunk_texts, padding=True, return_tensors='tf')\n",
    "        [predictions] = model(encoded_input)\n",
    "        predicted_index = tf.argmax(predictions, axis=2)\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(index) for index in predicted_index]\n",
    "        filled_seqs += [\"\".join(predict_token[1:np.sum(mask)-1]) \\\n",
    "                       for [predict_token, mask] in zip(predicted_tokens, encoded_input[\"attention_mask\"])]\n",
    "    return filled_seqs\n",
    "\n",
    "filled_seqs = mask_filling(df.masked.values.tolist()[0:100], batch_size=16)\n",
    "# df[\"filled\"] = filled_seqs\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "f51dfdcb-3b02-43ec-8522-cf5b9cb3c757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我对自己功能为力\n"
     ]
    }
   ],
   "source": [
    "text = \"我对自[MASK][MASK]能为力\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "[predictions] = model(encoded_input)\n",
    "\n",
    "predicted_index = tf.argmax(predictions[0], axis=1)\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\n",
    "print(\"\".join(predicted_token[1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "6c49aebd-10f5-403c-afe7-50378756df36"
   },
   "source": [
    "# Backtranslation\n",
    "Translate chinese to english, and translate back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "6e193f16-b519-4e81-821f-8170da4c057b"
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from aliyunsdkcore.client import AcsClient\n",
    "from aliyunsdkcore.acs_exception.exceptions import ClientException\n",
    "from aliyunsdkcore.acs_exception.exceptions import ServerException\n",
    "from aliyunsdkalimt.request.v20181012.TranslateGeneralRequest import TranslateGeneralRequest\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"/home/admin/workspace/.secret\")\n",
    "\n",
    "client = AcsClient(config[\"account xjx\"][\"access_key\"], \n",
    "                   config[\"account xjx\"][\"access_secret\"], \n",
    "                   'cn-hangzhou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "uuid": "82cc0fe6-1473-4bac-95c2-e6d1f2491a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 169 ms, sys: 986 ms, total: 1.15 s\n",
      "Wall time: 7.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "class BackTranslation:\n",
    "    def __init__(self):\n",
    "        self.bulk_size = 4800\n",
    "    \n",
    "    def back_translation(self, corpus):\n",
    "        translated = self._bulk_translate(corpus, from_lang = \"zh\", to_lang = \"en\")\n",
    "        back_translated = self._bulk_translate(translated, from_lang = \"en\", to_lang = \"zh\")\n",
    "        return back_translated\n",
    "    \n",
    "    def _bulk_translate(self, corpus, from_lang = \"zh\", to_lang = \"en\"):\n",
    "        translated = []\n",
    "        text = \"\"\n",
    "\n",
    "        def _do_translate(text, translated):\n",
    "            translated_text = self._translate(text.strip(), from_lang = from_lang, to_lang = to_lang)\n",
    "            translated +=  translated_text.split(\"\\n\")\n",
    "            \n",
    "        for seq in corpus:\n",
    "            if len(text + seq) >= self.bulk_size:\n",
    "                _do_translate(text, translated)\n",
    "                text = seq + \"\\n\"\n",
    "            else:\n",
    "                text += seq + \"\\n\"\n",
    "                \n",
    "        _do_translate(text, translated)\n",
    "        \n",
    "        return translated\n",
    "    \n",
    "    def _translate(self, text, from_lang = \"zh\", to_lang = \"en\"):\n",
    "        \"\"\"\n",
    "        The api of alimt has limit the maximum length of text to 5000 characters, maximum QPS to 50,\n",
    "        so we should send the request in several bulks, with less than 250000 characters in each bulk.\n",
    "        \"\"\"\n",
    "        request = TranslateGeneralRequest()\n",
    "        request.set_accept_format('json')\n",
    "\n",
    "        request.set_FormatType(\"text\")\n",
    "        request.set_SourceLanguage(from_lang)\n",
    "        request.set_TargetLanguage(to_lang)\n",
    "\n",
    "        request.set_SourceText(text)\n",
    "\n",
    "        response = client.do_action_with_exception(request)\n",
    "        response_json = json.loads(response)\n",
    "    \n",
    "        try:\n",
    "            translated = response_json[\"Data\"][\"Translated\"]\n",
    "            return translated\n",
    "        except:\n",
    "            print(response_json)\n",
    "            raise Exception(\"Response error\")\n",
    "    \n",
    "def parallelize(df, func):\n",
    "    partitions = multiprocessing.cpu_count()\n",
    "    df_splited = np.array_split(df, partitions)\n",
    "    df_splited = Parallel(\n",
    "        n_jobs=partitions\n",
    "    )(delayed(func)(df) for df in df_splited)\n",
    "    return np.concatenate(df_splited)\n",
    "\n",
    "back_translated = parallelize(df.seq.values, BackTranslation().back_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "uuid": "3fab60d5-a8b4-48ef-a5fb-42f49d56c977"
   },
   "outputs": [],
   "source": [
    "df[\"back_translated\"] = back_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "uuid": "b71604ac-1474-438a-b355-87afa03d27a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>masked</th>\n",
       "      <th>masked_rate</th>\n",
       "      <th>filled</th>\n",
       "      <th>back_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>这样才能酿出酒精度12的葡萄酒</td>\n",
       "      <td>这[MASK][MASK][MASK]酿[MASK][MASK][MASK][MASK][M...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>这样就可以酿造酒精含量为 12 的葡萄酒。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>新疆的鲜食葡萄很出名</td>\n",
       "      <td>新疆的鲜食[MASK]萄很出名</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新疆以其新鲜的葡萄而闻名。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>酿酒葡萄品质也是很好的</td>\n",
       "      <td>[MASK]酒[MASK][MASK]品[MASK][MASK]是很好[MASK]</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>酿酒葡萄质量也很好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>中国每年生产的葡萄酒</td>\n",
       "      <td>[MASK][MASK]每年[MASK]产的[MASK]萄酒</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>中国每年生产的葡萄酒</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>有很大的比重都是用新疆的酿酒葡萄汁酿造的</td>\n",
       "      <td>有很大的比重都是用新疆的[MASK]酒葡萄汁酿造的</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>很大一部分是用新疆的酿酒葡萄汁酿造的。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       seq                                             masked  \\\n",
       "1011       这样才能酿出酒精度12的葡萄酒  这[MASK][MASK][MASK]酿[MASK][MASK][MASK][MASK][M...   \n",
       "1012            新疆的鲜食葡萄很出名                                    新疆的鲜食[MASK]萄很出名   \n",
       "1013           酿酒葡萄品质也是很好的          [MASK]酒[MASK][MASK]品[MASK][MASK]是很好[MASK]   \n",
       "1014            中国每年生产的葡萄酒                     [MASK][MASK]每年[MASK]产的[MASK]萄酒   \n",
       "1015  有很大的比重都是用新疆的酿酒葡萄汁酿造的                          有很大的比重都是用新疆的[MASK]酒葡萄汁酿造的   \n",
       "\n",
       "      masked_rate filled        back_translated  \n",
       "1011     0.800000    NaN  这样就可以酿造酒精含量为 12 的葡萄酒。  \n",
       "1012     0.100000    NaN          新疆以其新鲜的葡萄而闻名。  \n",
       "1013     0.818182    NaN              酿酒葡萄质量也很好  \n",
       "1014     0.600000    NaN             中国每年生产的葡萄酒  \n",
       "1015     0.050000    NaN    很大一部分是用新疆的酿酒葡萄汁酿造的。  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "f6366be1-b9be-434e-a2f1-efb42af67844"
   },
   "source": [
    "# Word dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "uuid": "92d4570f-feeb-447f-8dcf-aab3cb374958"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>masked</th>\n",
       "      <th>masked_rate</th>\n",
       "      <th>filled</th>\n",
       "      <th>back_translated</th>\n",
       "      <th>dropped</th>\n",
       "      <th>dropped_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我对你仍有爱意</td>\n",
       "      <td>我对[MASK]仍有[MASK]意</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>我对我仍有同意</td>\n",
       "      <td>我仍然爱着你</td>\n",
       "      <td>我对仍有爱意</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我对自己无能为力</td>\n",
       "      <td>我对[MASK][MASK]无能为力</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>我对我，无能为力</td>\n",
       "      <td>我对自己无能为力。</td>\n",
       "      <td>我对自无能为力</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...</td>\n",
       "      <td>[MASK]一个同学[MASK][MASK]儿[MASK]那[MASK][MASK]小[MA...</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>那一个同学一个儿子那一一小孩二小级数学课老师讲话的时候一一个同桌一个儿子的小孩很贪玩也很寻常...</td>\n",
       "      <td>让我们谈谈一个同学。当他二年级的数学老师讲课时，这位同学和他的同桌一起玩。同样不寻常的是，数...</td>\n",
       "      <td>讲一个同学的事儿吧那年他小学二年数课老师讲课的时候学跟同桌一块玩儿小孩儿贪玩也寻常不寻常是数...</td>\n",
       "      <td>0.094241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我看了上帝之眼感觉还不错</td>\n",
       "      <td>我[MASK]了[MASK][MASK][MASK]眼感[MASK]还[MASK]错</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>我看到了上帝的眼睛，感觉很好。</td>\n",
       "      <td>我了帝之眼感觉不错</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>不喜欢那些讲技术的摄影书</td>\n",
       "      <td>[MASK]喜欢那些[MASK][MASK][MASK]的[MASK][MASK]书</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>我不喜欢那些谈论技术的摄影书籍。</td>\n",
       "      <td>不喜欢些讲技术的摄影书</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 seq  \\\n",
       "0                                            我对你仍有爱意   \n",
       "1                                           我对自己无能为力   \n",
       "2  讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常...   \n",
       "3                                       我看了上帝之眼感觉还不错   \n",
       "4                                       不喜欢那些讲技术的摄影书   \n",
       "\n",
       "                                              masked  masked_rate  \\\n",
       "0                                  我对[MASK]仍有[MASK]意     0.285714   \n",
       "1                                 我对[MASK][MASK]无能为力     0.250000   \n",
       "2  [MASK]一个同学[MASK][MASK]儿[MASK]那[MASK][MASK]小[MA...     0.439791   \n",
       "3         我[MASK]了[MASK][MASK][MASK]眼感[MASK]还[MASK]错     0.583333   \n",
       "4         [MASK]喜欢那些[MASK][MASK][MASK]的[MASK][MASK]书     0.750000   \n",
       "\n",
       "                                              filled  \\\n",
       "0                                            我对我仍有同意   \n",
       "1                                           我对我，无能为力   \n",
       "2  那一个同学一个儿子那一一小孩二小级数学课老师讲话的时候一一个同桌一个儿子的小孩很贪玩也很寻常...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                     back_translated  \\\n",
       "0                                             我仍然爱着你   \n",
       "1                                          我对自己无能为力。   \n",
       "2  让我们谈谈一个同学。当他二年级的数学老师讲课时，这位同学和他的同桌一起玩。同样不寻常的是，数...   \n",
       "3                                    我看到了上帝的眼睛，感觉很好。   \n",
       "4                                   我不喜欢那些谈论技术的摄影书籍。   \n",
       "\n",
       "                                             dropped  dropped_rate  \n",
       "0                                             我对仍有爱意      0.142857  \n",
       "1                                            我对自无能为力      0.125000  \n",
       "2  讲一个同学的事儿吧那年他小学二年数课老师讲课的时候学跟同桌一块玩儿小孩儿贪玩也寻常不寻常是数...      0.094241  \n",
       "3                                          我了帝之眼感觉不错      0.250000  \n",
       "4                                        不喜欢些讲技术的摄影书      0.083333  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_dropping(text):\n",
    "    \"\"\"\n",
    "    Randomly drop some words in the sequence\n",
    "    \"\"\"\n",
    "    seq = list(text)\n",
    "    text_len = len(text)\n",
    "    k = random.choice([1] + list(range(1, int(text_len/3))))\n",
    "    for i in random.choices(range(text_len), k = k):\n",
    "        seq[i] = \"\"\n",
    "    dropped_rate = k/text_len\n",
    "    dropped = \"\".join(seq)\n",
    "    return pd.Series([dropped, dropped_rate], index=[\"dropped\", \"dropped_rate\"])\n",
    "\n",
    "dropped = df.seq.apply(word_dropping)\n",
    "df[\"dropped\"] = dropped.dropped\n",
    "df[\"dropped_rate\"] = dropped.dropped_rate\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "fae512ee-8af9-4e9b-ad2e-6deb2cb9d8a8"
   },
   "source": [
    "# Blend these all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "uuid": "6fa15dc0-4c78-40cd-8808-24bdea434a1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我对你仍有爱意',\n",
       " '我对自己无能为力',\n",
       " '讲一个同学的事儿吧那年他小学二年级数学课老师讲课的时候同学跟同桌一块儿玩儿小孩儿贪玩也很寻常不寻常的是数学老师勃然大怒抓起我同学的衣领拎着他就直接走出教室教室是在四楼这老师竟然直接就把他悬在了走道护杆外的半空同学直接吓尿了（是真的尿）这老师还在骂骂咧咧的威胁以后还敢不敢上课瞎捣蛋结局同学还是安然被“收了回来”但从此他不敢在数学课上放肆从此他的数学成绩没有突破过及格线一个好的老师传',\n",
       " '我看了上帝之眼感觉还不错',\n",
       " '不喜欢那些讲技术的摄影书']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"./webtext2019zh/web_text_zh_train_sample.json\", \"r\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "data = map(json.loads, content)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "text = \"\\n\".join(data.content.values)\n",
    "references = cut_sentences(text)\n",
    "references[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "uuid": "0b0e24e0-489b-4d46-9ca7-8516121aae3c"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "uuid": "c0f178ba-d308-4710-8383-43a75e904d78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Apply mask filling ...\n",
      "2020-08-14 12:18:01,172 INFO     Apply mask filling ...\n",
      "INFO:root:Apply back translation ...\n",
      "2020-08-14 12:18:01,173 INFO     Apply back translation ...\n",
      "INFO:root:Dropped 85 samples\n",
      "2020-08-14 12:18:05,964 INFO     Dropped 85 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 559 ms, sys: 11.6 ms, total: 570 ms\n",
      "Wall time: 5.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def make_candidates(references):\n",
    "    \"\"\"\n",
    "    30% with mask filling rule1: scored by masked_rate\n",
    "    30% with mask filling rule2: scored by masked_rate\n",
    "    30% with back translation and word dropping: scored by dropped rate\n",
    "    10% with back translation: score 0.98\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    candidates: Generated candidates with the same length of references\n",
    "    scores: Arbitrary scores\n",
    "    \"\"\"\n",
    "    # Do not modify input params\n",
    "    references = references.copy()\n",
    "    random.shuffle(references)\n",
    "    refs = []\n",
    "    \n",
    "    ref_len = len(references)\n",
    "\n",
    "    # Apply mask filling\n",
    "    logger.info(\"Apply mask filling ...\")\n",
    "    mf1_len = mf2_len = int(ref_len*0.3)\n",
    "    candidates = []\n",
    "    scores = []\n",
    "\n",
    "    # Mask filling\n",
    "    mf1 = map(mask_replacing, references[:mf1_len])\n",
    "    refs += references[:mf1_len]\n",
    "    del references[:mf1_len]\n",
    "    mf2 = map(mask_replacing2, references[:mf2_len])\n",
    "    refs += references[:mf2_len]\n",
    "    del references[:mf2_len]\n",
    "    mf = pd.DataFrame(list(mf1) + list(mf2))\n",
    "    mf_filled = mf.masked.apply(mask_filling)\n",
    "\n",
    "    candidates = mf_filled.tolist()\n",
    "    scores += (1 - mf.masked_rate).values.tolist()\n",
    "    \n",
    "    # Back translation (bt)\n",
    "    logger.info(\"Apply back translation ...\")\n",
    "    bt = parallelize(references, lambda refs: BackTranslation().back_translation(refs))\n",
    "    # Drop samples where refs and back translationed excactly same\n",
    "    df_bt = pd.DataFrame({\n",
    "        \"refs\": references,\n",
    "        \"bt\": bt\n",
    "    })\n",
    "    df_bt = df_bt[df_bt.refs != df_bt.bt]\n",
    "\n",
    "    # Replace references and bt for later use\n",
    "    logger.info(\"Dropped {} samples\".format(len(bt) - df_bt.shape[0]))\n",
    "    refs += df_bt.refs.tolist()\n",
    "    references = df_bt.refs.tolist()\n",
    "    bt = df_bt.bt.tolist()\n",
    "    \n",
    "    # Apply 30% with word dropping\n",
    "    wd_len = int(df_bt.shape[0] * 0.75)\n",
    "    bt_dropped = map(word_dropping, bt[:wd_len])\n",
    "    bt_dropped = pd.DataFrame(bt_dropped)\n",
    "    candidates += bt_dropped.dropped.tolist()\n",
    "    scores += (1 - bt_dropped.dropped_rate).tolist()\n",
    "    \n",
    "    del bt[:wd_len]\n",
    "    candidates += bt\n",
    "    scores += [0.98] * len(bt)\n",
    "\n",
    "    return refs, candidates, scores\n",
    "   \n",
    "[refs, candidates, scores] = make_candidates(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "uuid": "d396e8cc-5520-4f0c-a3d3-9728564b96bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 931, 931)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refs), len(candidates), len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "uuid": "c6f357ea-280f-4200-8ae3-76da1e6b65f3"
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({\n",
    "    \"reference\": refs,\n",
    "    \"candidate\": candidates,\n",
    "    \"score\": scores\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "uuid": "3a04f273-5038-4f26-928d-9a11273a2c78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>candidate</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>财企2007194号给你们参考：一研发活动直接消耗的材料燃料和动力费用</td>\n",
       "      <td>财企业007##19##4号给你们参考：一研发活动直接消耗的材料燃料和动力费用</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>反正黄赌毒各种我价值观上不好的行为我都猜了一遍</td>\n",
       "      <td>反正黄赌上各种我价值观上不好的行为我都猜了一遍</td>\n",
       "      <td>0.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>看到这一幕我简直兴奋得要上天了更新于上一个答案几个小时后剧透醒目论这个世界上还有什么比妖怪夫...</td>\n",
       "      <td>看到这一幕，我太兴奋了，我要去天堂了。我更新了最后一个答案几个小时后，剧引人注目。世界上没有...</td>\n",
       "      <td>0.953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>你觉得我们要不要使用千度的代码机器或者370的代码机器</td>\n",
       "      <td>你认为我们应该用几千台码机还是 70 台码机</td>\n",
       "      <td>0.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>叙利亚西北部伊德利卜省出现的使用化学武器造成大规模平民伤亡</td>\n",
       "      <td>叙利亚西北部Idleb省使用化学武器造成大规模民伤亡</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>我也没找到下载的链接）：1支持JVM（支持Java应用）2不开源3号称十万个应用4能在移动设备</td>\n",
       "      <td>我也有找到下载链接):1 支持JVM 支持Java应用程序) 2 不开源 3 声称 100,...</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>二企业在职研发人员的工资奖金津贴补贴社会保险费住房公积金等人工费用以及外聘研发人员的劳务费</td>\n",
       "      <td>第二，企业在职R &amp; D人员的工资和金补贴社会保险等劳动成本，住房公积金和外部研发人员的劳动...</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>女主盯着我看了半天我以为我衣服穿错了</td>\n",
       "      <td>女主角盯着我看了很久。我以为我穿衣服了。</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>先说一说毁灭之锤奥格瑞姆你个锤子！这是黑暗之门后各位兽人酋长发自内心的咆哮这个是还在当角斗士的萨尔</td>\n",
       "      <td>先说说毁灭之锤吧，奥格丽德，你这个锤子!这是黑暗之门后兽人首领内心的咆哮。这是萨尔，他仍然是...</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>让一些不牛逼的大学也有一些学术性理论性很强的专业</td>\n",
       "      <td>让一些不牛逼的大学有一些学术和理论专业。</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>都是渣渣！英国留学期间一直在独立咖啡馆当咖啡师</td>\n",
       "      <td>都是渣!在英学习期间，我一直在一家独立咖啡馆做咖啡师。</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>脑海里总要找话题消除木讷冷场</td>\n",
       "      <td>在我的脑海里，我是需要找一个话题来消除沉闷的沉默。</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>认识的时候他几千粉我一万粉</td>\n",
       "      <td>当我遇见他时，他有几千个粉，我有 10,000 个粉末。</td>\n",
       "      <td>0.965517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>答与恐怖疯子对峙的时候这分割线是这么用的不</td>\n",
       "      <td>答: 这在对抗恐怖分子疯子时使用这条分界线吗？</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>台下黑压压的领导与同学四五百号人</td>\n",
       "      <td>台下黑压 400 或 50 的领导和同学</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>和白诗诗沟通的过程中有一个非常明显的感觉</td>\n",
       "      <td>在与白诗交的过程中，有一种非常明显的感觉。</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>我觉得是这样的性格不做作不装逼不低头再加上：爱谁谁</td>\n",
       "      <td>我觉是这样的性格，不做作，不做作，不鞠躬，再加上: 你爱谁</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>却变成了收纳重灾区！你会承认这是家吗</td>\n",
       "      <td>但它已经成为存的重灾区!你会承认这是家吗</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>星巴克的咖啡机都是为了降低对咖啡师的技术要求而特制的而Costa相对更有英国人的慢节奏</td>\n",
       "      <td>星巴克咖啡机是专门降低对咖啡师的技术要求而制作的，而Costa在英国则相对较慢。</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>同时我暗自揣测</td>\n",
       "      <td>同时，我暗暗推测</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>是六个类似于学前班的小孩的团体</td>\n",
       "      <td>它是一组六个孩子类似于学龄前儿童。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>自我挣扎了几年还是从了某个姑娘</td>\n",
       "      <td>奋斗了几年，我还是向一个女孩学习。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>无论是泉水勾还是overpass的不合理双架</td>\n",
       "      <td>无论是泉水钩还是立交桥不合理的双框</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>严格的讲属于社会科学</td>\n",
       "      <td>严格地说，它属于社会科学。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>海军进行曲也不错</td>\n",
       "      <td>海军进行曲也不错</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>直到转学了才受到打击</td>\n",
       "      <td>直到我转到另一所学校，它才被击中。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>知道我籍贯后</td>\n",
       "      <td>知道我的籍贯后</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>建议大家去这个问题下面寻找答案</td>\n",
       "      <td>我建议你去下面的问题找到答案。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>呸呸呸</td>\n",
       "      <td>呸</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>但是一定有人在仰望星空</td>\n",
       "      <td>但是一定有人在仰望星空</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>只应该出现在名校</td>\n",
       "      <td>应该只出现在名校</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>3碎觉觉了</td>\n",
       "      <td>3 感觉好像坏了</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>那有没有SIN系统啊</td>\n",
       "      <td>有罪恶系统吗？</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>iPhone7销量如何</td>\n",
       "      <td>IPhone7 销量怎么样</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>无一例外成住坏空</td>\n",
       "      <td>毫无例外，它变成了一个糟糕的地方</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>comarchives50914</td>\n",
       "      <td>Comarchives50914</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>略微偏意式一点</td>\n",
       "      <td>略偏</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>所以不妨从源头着手</td>\n",
       "      <td>所以我们不妨从源头开始。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>GPU没有datastructure</td>\n",
       "      <td>GPU没有datastructure</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>到我这里的时候便发生了冲突</td>\n",
       "      <td>当我来到这里时，发生了冲突。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>然后就开始很喜欢刷这种刷三观的剧了</td>\n",
       "      <td>然后我开始喜欢刷这种剧</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>前者是最原始的文献</td>\n",
       "      <td>前者是最原始的文学。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>不够委婉</td>\n",
       "      <td>不够委婉</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>必须同时使用配套的千度手机代码机器</td>\n",
       "      <td>必须同时使用支持的千度手机码机</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>这一点已经在我心里一锤定音</td>\n",
       "      <td>这已经在我心里敲定了。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>作为一档面向大众的辩论节目</td>\n",
       "      <td>作为一个公共辩论项目</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>重心反而会加重在健侧下肢</td>\n",
       "      <td>相反，重心会在健康的下肢增加。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>最简单的办法是：物化怎样与和自己不太投契的人打交道</td>\n",
       "      <td>最简单的方法是实现如何与不同意自己的人打交道。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>传说中的蓝色没有出现</td>\n",
       "      <td>传说中的蓝色没有出现</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>3月31日</td>\n",
       "      <td>3 月 31 日</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>基本上了解了整个这个旨在做iOSapp团队的全貌</td>\n",
       "      <td>基本了解iOSapp团队的全貌</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>一些事物</td>\n",
       "      <td>一些事情</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>克莱尔要的是万</td>\n",
       "      <td>克莱尔想要的是Wan</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>黑五发自拍感觉和大三在上海毫无区别</td>\n",
       "      <td>黑色五毛自拍的感觉和上海的junior没什么区别。</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>angular</td>\n",
       "      <td>角</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>4月4日</td>\n",
       "      <td>4 月 4 日</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>酿酒葡萄品质也是很好的</td>\n",
       "      <td>酿酒葡萄质量也很好</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>得了罕见病是什么感受</td>\n",
       "      <td>有一种罕见病的感觉是什么？</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>大H也是</td>\n",
       "      <td>大H也是</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>会在生成的代码中强行加入特殊指令</td>\n",
       "      <td>将强制向生成的代码添加特殊指令</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reference  \\\n",
       "3                   财企2007194号给你们参考：一研发活动直接消耗的材料燃料和动力费用   \n",
       "49                              反正黄赌毒各种我价值观上不好的行为我都猜了一遍   \n",
       "615   看到这一幕我简直兴奋得要上天了更新于上一个答案几个小时后剧透醒目论这个世界上还有什么比妖怪夫...   \n",
       "644                         你觉得我们要不要使用千度的代码机器或者370的代码机器   \n",
       "656                       叙利亚西北部伊德利卜省出现的使用化学武器造成大规模平民伤亡   \n",
       "677     我也没找到下载的链接）：1支持JVM（支持Java应用）2不开源3号称十万个应用4能在移动设备   \n",
       "685       二企业在职研发人员的工资奖金津贴补贴社会保险费住房公积金等人工费用以及外聘研发人员的劳务费   \n",
       "696                                  女主盯着我看了半天我以为我衣服穿错了   \n",
       "714   先说一说毁灭之锤奥格瑞姆你个锤子！这是黑暗之门后各位兽人酋长发自内心的咆哮这个是还在当角斗士的萨尔   \n",
       "721                            让一些不牛逼的大学也有一些学术性理论性很强的专业   \n",
       "726                             都是渣渣！英国留学期间一直在独立咖啡馆当咖啡师   \n",
       "750                                      脑海里总要找话题消除木讷冷场   \n",
       "751                                       认识的时候他几千粉我一万粉   \n",
       "753                               答与恐怖疯子对峙的时候这分割线是这么用的不   \n",
       "755                                    台下黑压压的领导与同学四五百号人   \n",
       "804                                和白诗诗沟通的过程中有一个非常明显的感觉   \n",
       "807                           我觉得是这样的性格不做作不装逼不低头再加上：爱谁谁   \n",
       "865                                  却变成了收纳重灾区！你会承认这是家吗   \n",
       "894         星巴克的咖啡机都是为了降低对咖啡师的技术要求而特制的而Costa相对更有英国人的慢节奏   \n",
       "912                                             同时我暗自揣测   \n",
       "913                                     是六个类似于学前班的小孩的团体   \n",
       "914                                     自我挣扎了几年还是从了某个姑娘   \n",
       "915                              无论是泉水勾还是overpass的不合理双架   \n",
       "916                                          严格的讲属于社会科学   \n",
       "917                                            海军进行曲也不错   \n",
       "918                                          直到转学了才受到打击   \n",
       "919                                              知道我籍贯后   \n",
       "920                                     建议大家去这个问题下面寻找答案   \n",
       "921                                                 呸呸呸   \n",
       "922                                         但是一定有人在仰望星空   \n",
       "...                                                 ...   \n",
       "986                                            只应该出现在名校   \n",
       "987                                               3碎觉觉了   \n",
       "988                                          那有没有SIN系统啊   \n",
       "989                                         iPhone7销量如何   \n",
       "990                                            无一例外成住坏空   \n",
       "991                                    comarchives50914   \n",
       "992                                             略微偏意式一点   \n",
       "993                                           所以不妨从源头着手   \n",
       "994                                  GPU没有datastructure   \n",
       "995                                       到我这里的时候便发生了冲突   \n",
       "996                                   然后就开始很喜欢刷这种刷三观的剧了   \n",
       "997                                           前者是最原始的文献   \n",
       "998                                                不够委婉   \n",
       "999                                   必须同时使用配套的千度手机代码机器   \n",
       "1000                                      这一点已经在我心里一锤定音   \n",
       "1001                                      作为一档面向大众的辩论节目   \n",
       "1002                                       重心反而会加重在健侧下肢   \n",
       "1003                          最简单的办法是：物化怎样与和自己不太投契的人打交道   \n",
       "1004                                         传说中的蓝色没有出现   \n",
       "1005                                              3月31日   \n",
       "1006                           基本上了解了整个这个旨在做iOSapp团队的全貌   \n",
       "1007                                               一些事物   \n",
       "1008                                            克莱尔要的是万   \n",
       "1009                                  黑五发自拍感觉和大三在上海毫无区别   \n",
       "1010                                            angular   \n",
       "1011                                               4月4日   \n",
       "1012                                        酿酒葡萄品质也是很好的   \n",
       "1013                                         得了罕见病是什么感受   \n",
       "1014                                               大H也是   \n",
       "1015                                   会在生成的代码中强行加入特殊指令   \n",
       "\n",
       "                                              candidate     score  \n",
       "3               财企业007##19##4号给你们参考：一研发活动直接消耗的材料燃料和动力费用  0.971429  \n",
       "49                              反正黄赌上各种我价值观上不好的行为我都猜了一遍  0.956522  \n",
       "615   看到这一幕，我太兴奋了，我要去天堂了。我更新了最后一个答案几个小时后，剧引人注目。世界上没有...  0.953125  \n",
       "644                              你认为我们应该用几千台码机还是 70 台码机  0.956522  \n",
       "656                          叙利亚西北部Idleb省使用化学武器造成大规模民伤亡  0.962963  \n",
       "677   我也有找到下载链接):1 支持JVM 支持Java应用程序) 2 不开源 3 声称 100,...  0.970588  \n",
       "685   第二，企业在职R & D人员的工资和金补贴社会保险等劳动成本，住房公积金和外部研发人员的劳动...  0.961538  \n",
       "696                                女主角盯着我看了很久。我以为我穿衣服了。  0.952381  \n",
       "714   先说说毁灭之锤吧，奥格丽德，你这个锤子!这是黑暗之门后兽人首领内心的咆哮。这是萨尔，他仍然是...  0.980769  \n",
       "721                                让一些不牛逼的大学有一些学术和理论专业。  0.952381  \n",
       "726                         都是渣!在英学习期间，我一直在一家独立咖啡馆做咖啡师。  0.964286  \n",
       "750                           在我的脑海里，我是需要找一个话题来消除沉闷的沉默。  0.961538  \n",
       "751                        当我遇见他时，他有几千个粉，我有 10,000 个粉末。  0.965517  \n",
       "753                             答: 这在对抗恐怖分子疯子时使用这条分界线吗？  0.958333  \n",
       "755                                台下黑压 400 或 50 的领导和同学  0.952381  \n",
       "804                               在与白诗交的过程中，有一种非常明显的感觉。  0.954545  \n",
       "807                       我觉是这样的性格，不做作，不做作，不鞠躬，再加上: 你爱谁  0.966667  \n",
       "865                                但它已经成为存的重灾区!你会承认这是家吗  0.952381  \n",
       "894            星巴克咖啡机是专门降低对咖啡师的技术要求而制作的，而Costa在英国则相对较慢。  0.975610  \n",
       "912                                            同时，我暗暗推测  1.000000  \n",
       "913                                   它是一组六个孩子类似于学龄前儿童。  1.000000  \n",
       "914                                   奋斗了几年，我还是向一个女孩学习。  1.000000  \n",
       "915                                   无论是泉水钩还是立交桥不合理的双框  1.000000  \n",
       "916                                       严格地说，它属于社会科学。  1.000000  \n",
       "917                                            海军进行曲也不错  1.000000  \n",
       "918                                   直到我转到另一所学校，它才被击中。  1.000000  \n",
       "919                                             知道我的籍贯后  1.000000  \n",
       "920                                     我建议你去下面的问题找到答案。  1.000000  \n",
       "921                                                   呸  1.000000  \n",
       "922                                         但是一定有人在仰望星空  1.000000  \n",
       "...                                                 ...       ...  \n",
       "986                                            应该只出现在名校  1.000000  \n",
       "987                                            3 感觉好像坏了  1.000000  \n",
       "988                                             有罪恶系统吗？  1.000000  \n",
       "989                                       IPhone7 销量怎么样  1.000000  \n",
       "990                                    毫无例外，它变成了一个糟糕的地方  1.000000  \n",
       "991                                    Comarchives50914  1.000000  \n",
       "992                                                  略偏  1.000000  \n",
       "993                                        所以我们不妨从源头开始。  1.000000  \n",
       "994                                  GPU没有datastructure  1.000000  \n",
       "995                                      当我来到这里时，发生了冲突。  1.000000  \n",
       "996                                         然后我开始喜欢刷这种剧  1.000000  \n",
       "997                                          前者是最原始的文学。  1.000000  \n",
       "998                                                不够委婉  1.000000  \n",
       "999                                     必须同时使用支持的千度手机码机  1.000000  \n",
       "1000                                        这已经在我心里敲定了。  1.000000  \n",
       "1001                                         作为一个公共辩论项目  1.000000  \n",
       "1002                                    相反，重心会在健康的下肢增加。  1.000000  \n",
       "1003                            最简单的方法是实现如何与不同意自己的人打交道。  1.000000  \n",
       "1004                                         传说中的蓝色没有出现  1.000000  \n",
       "1005                                           3 月 31 日  1.000000  \n",
       "1006                                    基本了解iOSapp团队的全貌  1.000000  \n",
       "1007                                               一些事情  1.000000  \n",
       "1008                                         克莱尔想要的是Wan  1.000000  \n",
       "1009                          黑色五毛自拍的感觉和上海的junior没什么区别。  1.000000  \n",
       "1010                                                  角  1.000000  \n",
       "1011                                            4 月 4 日  1.000000  \n",
       "1012                                          酿酒葡萄质量也很好  1.000000  \n",
       "1013                                      有一种罕见病的感觉是什么？  1.000000  \n",
       "1014                                               大H也是  1.000000  \n",
       "1015                                    将强制向生成的代码添加特殊指令  1.000000  \n",
       "\n",
       "[123 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset.score>0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "uuid": "273a610e-1eb6-474b-be63-fbe8f7089526"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv(\"./data_generated/dataset.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "uuid": "8512ba3a-27f6-47a4-b25b-faebaac2dbf1"
   },
   "outputs": [],
   "source": [
    "# @TODO randomly mix other 0 scored references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "uuid": "ab086287-647b-4602-9a95-de3fb880a693"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data_generated/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "uuid": "28ef2876-d1ec-476d-a986-3c4fe28e1717"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv(\"./data_generated/dataset.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "uuid": "9b742f54-649b-4eb4-8b12-d8940aef5bf8"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv(\"./data_generated/dataset.csv\", index=None, header=None, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "uuid": "0cfc09f2-9fdb-48b3-adb8-25a56480f3b2"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data_generated/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "uuid": "d748f6d2-11b1-43e1-9ae3-908e3aee6043"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "uuid": "9b31023f-b0ff-4609-8bd9-fa17a67f55d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference\n"
     ]
    }
   ],
   "source": [
    "dataset.apply(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "uuid": "b28b8d09-8401-42a7-ba01-584464bdb17c"
   },
   "outputs": [],
   "source": [
    "row = dataset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "uuid": "82cdfebf-e5e9-4a1e-a87f-760c68578540"
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "def save_data(dataset):\n",
    "    \"\"\"\n",
    "    Save data to csv and jsonl\n",
    "    jsonl example: {\"candidate\":\"吴承恩是著名文学家\",\"reference\":\"吴承恩是著名文学家\",\"score\":1}\n",
    "    \"\"\"\n",
    "    csv_file = \"./data_generated/dataset.csv\"\n",
    "    jsonl_file = \"./data_generated/dataset.jsonl\"\n",
    "    \n",
    "    mode = \"w\"\n",
    "    if path.exists(csv_file):\n",
    "        mode = \"a\"\n",
    "        \n",
    "    dataset.to_csv(csv_file, index=None, header=None, mode=mode)\n",
    "    \n",
    "    def write_row(f, row):\n",
    "        f.write(row.to_json(force_ascii=False) + \"\\n\")\n",
    "    \n",
    "    with open(jsonl_file, mode) as f:\n",
    "        dataset.apply(lambda row: write_row(f, row), axis=1)\n",
    "        \n",
    "save_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "uuid": "7668dafa-ba98-4a4b-b36b-8a22fa6fc88c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = 0\n",
    "batch_size = 30\n",
    "content = []\n",
    "for i in range(100):\n",
    "    if i > checkpoint and i <= (checkpoint + batch_size):\n",
    "        content.append(i)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "uuid": "51e50529-53ad-423b-94e2-26ba364c036f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n",
      "21 42\n",
      "21 63\n",
      "21 84\n",
      "16 None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def readfile(filename, checkpoint = 0, batch_size = 300):\n",
    "    content = []\n",
    "    i = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        while True:\n",
    "            i += 1\n",
    "            if i > checkpoint and i <= (checkpoint + batch_size):\n",
    "                line = f.readline()\n",
    "                if line != \"\":\n",
    "                    content.append(line)\n",
    "                else:\n",
    "                    checkpoint = None\n",
    "                    break\n",
    "            elif i <= checkpoint:\n",
    "                next(f)\n",
    "            else:\n",
    "                checkpoint = i - 1\n",
    "                break\n",
    "    return content, checkpoint\n",
    "\n",
    "checkpoint = 0\n",
    "for i in range(10):\n",
    "    [content, checkpoint] = readfile(\"./webtext2019zh/web_text_zh_train_sample.json\", \n",
    "                                     checkpoint = checkpoint, \n",
    "                                     batch_size = 21)\n",
    "    time.sleep(0.5)\n",
    "    print(len(content), checkpoint)\n",
    "    \n",
    "    if checkpoint is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "uuid": "451b6cff-697d-4883-896b-f51366b26848"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"qid\": 58303597, \"title\": \"空姐的行李箱里有什么？\", \"desc\": \"\", \"topic\": \"空乘\", \"star\": 19, \"content\": \"想知道吗？ 我打开给你看啊。\", \"answer_id\": 165171803, \"answerer_tags\": \"斯人若彩虹，遇上方知有。\"}\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "uuid": "84c1f918-f4bb-49a7-a09c-06673fea2201"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "uuid": "ca2f3e98-b974-4ac0-a28c-1366baf8749d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "4691b843-93db-4346-815b-1b04ebe6d3c1"
   },
   "source": [
    "# Post process\n",
    "\n",
    "Generate negetive samples scored 0 and positive samples scored 1,\n",
    "which each part has the 10% size of origin dataset,\n",
    "split into train/dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "08b686e3-6824-42b4-a218-2fd4bdf5b98a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "9286df21-75eb-4951-b98c-ac34d2cdc6a0"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data_generated/dataset.csv\", \n",
    "                      header=None, \n",
    "                      names=[\"reference\", \"candidate\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "uuid": "c8c4c8b3-81e9-451a-8e92-a9b148c8ee06"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "uuid": "9ad954c0-96f5-4d91-b0fc-98d11ac2a7fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9076109, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "7464d16a-2f91-4b4e-8714-3a57e53f4306"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9.077648e+06\n",
       "mean     7.068054e-01\n",
       "std      2.366038e-01\n",
       "min      0.000000e+00\n",
       "25%      5.600000e-01\n",
       "50%      7.857143e-01\n",
       "75%      8.888889e-01\n",
       "max      9.964286e-01\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "uuid": "c0f8c383-8ae5-4833-8e5c-033a05025100"
   },
   "outputs": [],
   "source": [
    "# Replace [UNK]\n",
    "dataset[\"candidate\"] = dataset[\"candidate\"].str.replace(\"\\[UNK\\]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "uuid": "866769a1-130e-4252-bb1b-a4e676917503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 40s, sys: 1.98 s, total: 29min 42s\n",
      "Wall time: 29min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 乘以 bleu 系数\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "bleu_score = dataset.apply(lambda df: sentence_bleu([df.reference], df.candidate), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "uuid": "6786c240-be8f-407f-9da2-11dcb5449565"
   },
   "outputs": [],
   "source": [
    "dataset[\"raw_score\"] = dataset[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "uuid": "faf9a6a3-fb5e-475b-b329-d2e34ab9d8c5"
   },
   "outputs": [],
   "source": [
    "dataset[\"score\"] = dataset[\"raw_score\"]*bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "uuid": "c1c20527-2a93-4c90-a093-071035e66942"
   },
   "outputs": [],
   "source": [
    "dataset[\"score\"] = dataset[\"score\"].round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "uuid": "6ef70c42-d8fa-40e7-a272-79db3eed942e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9.076109e+06\n",
       "mean     2.362756e-01\n",
       "std      2.755862e-01\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      1.312070e-01\n",
       "75%      4.355420e-01\n",
       "max      9.947920e-01\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "uuid": "f674b6f6-f4e9-42e9-a1dd-e392bf0e1ee8"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv(\"./data_generated/dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "uuid": "ebb856e0-decf-487f-be2c-80eed9e8b602"
   },
   "outputs": [],
   "source": [
    "n_neg_samples = int(dataset.shape[0]/5)\n",
    "n_pos_samples = int(dataset.shape[0]/5)\n",
    "\n",
    "neg_samples = dataset.sample(n_neg_samples).reset_index(drop=True)\n",
    "neg_samples[\"candidate\"] = dataset[\"reference\"].sample(n_neg_samples).values\n",
    "neg_samples[\"score\"] = 0\n",
    "\n",
    "pos_samples = dataset.sample(n_neg_samples).reset_index(drop=True)\n",
    "pos_samples[\"candidate\"] = pos_samples[\"reference\"].values\n",
    "pos_samples[\"score\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "uuid": "6834c706-b588-40f6-b1db-399e18df02a1"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    dataset, neg_samples, pos_samples\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "uuid": "1e3a6edb-7163-482c-8805-415825053b21"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "uuid": "4d7cf5f6-6218-4b17-a65a-58466991160a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, dev_set = train_test_split(df, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "uuid": "bcd95b43-29bd-46a2-9b37-dac7dfafd71f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.258010e+07\n",
       "mean     6.477867e-01\n",
       "std      3.465629e-01\n",
       "min      0.000000e+00\n",
       "25%      4.000000e-01\n",
       "50%      7.857143e-01\n",
       "75%      9.285714e-01\n",
       "max      1.000000e+00\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "uuid": "a02b01f4-8880-4aef-a116-a7a5d7c3136f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12580095, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "uuid": "56b91575-b571-4f5f-a076-4b0bb285c3ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.4 s, sys: 9.09 s, total: 41.5 s\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Write to jsonl file\n",
    "\n",
    "train_set.to_json(\"./data_generated/rating_train.jsonl\", orient='records', force_ascii=False, lines=True)\n",
    "dev_set.to_json(\"./data_generated/rating_dev.jsonl\", orient='records', force_ascii=False, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "25d4236e-f176-4a5a-afda-3a8e22b9c59f"
   },
   "source": [
    "# 构建极性词库来调优模型表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "uuid": "315d5061-aed3-4a01-ad00-e859cac0dfb0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df_finetune = pd.DataFrame()\n",
    "\n",
    "# 读取反义词\n",
    "with open(\"./homebrewed/dict_antonym.txt\") as f:\n",
    "    dict_antonym = f.readlines()\n",
    "    dict_antonym = map(lambda x: re.split(\"(--|——)\", x.strip())[0::2] + [0], dict_antonym)\n",
    "    dict_antonym = list(dict_antonym)\n",
    "    df_finetune = df_finetune.append(dict_antonym, ignore_index=True)\n",
    "    \n",
    "# 读取同义词\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "def random_sample(candidates):\n",
    "    \"\"\"\n",
    "    Randomly get samples from candidates, each sample has a pair of words\n",
    "    \"\"\"\n",
    "    k = int(np.log2(len(candidates) * (len(candidates) - 1)))\n",
    "    samples = [\"--\".join(random.sample(candidates, 2)) for _ in range(k)]\n",
    "    samples = set(samples)\n",
    "    samples = map(lambda x: x.split(\"--\"), samples)\n",
    "    return list(samples)\n",
    "    \n",
    "with open(\"./homebrewed/dict_synonym.txt\") as f:\n",
    "    dict_synonym = f.readlines()\n",
    "    dict_synonym = map(lambda x: x.strip()[9:], dict_synonym)\n",
    "    dict_synonym = map(lambda x: x.split(\" \"), dict_synonym)\n",
    "    dict_synonym = filter(lambda x: len(x) > 1, dict_synonym)\n",
    "    dict_synonym = map(lambda x: random_sample(x), dict_synonym)\n",
    "    dict_synonym = chain.from_iterable(dict_synonym)\n",
    "    dict_synonym = map(lambda x: x + [0.9], dict_synonym)\n",
    "    dict_synonym = list(dict_synonym)\n",
    "    \n",
    "    df_finetune = df_finetune.append(dict_synonym, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "uuid": "0e749461-6c72-4b9c-836d-07eeeb1f888a"
   },
   "outputs": [],
   "source": [
    "# 增加 LC 数据\n",
    "df_lc = pd.read_json(\"./data_generated/LC.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "766b71f6-feb7-4d56-b8c6-c542a5121728"
   },
   "outputs": [],
   "source": [
    "bleu_score = df_lc.apply(lambda df: sentence_bleu([df.sentence1], df.sentence2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "uuid": "994bf4fa-2ddf-49f7-a817-3cace029f2a2"
   },
   "outputs": [],
   "source": [
    "# 调整 score 系数\n",
    "df_lc[\"score\"] = df_lc[\"gold_label\"] * 0.5 + 0.4\n",
    "df_lc[\"score\"] = df_lc[\"score\"] * bleu_score\n",
    "df_lc[\"score\"] = df_lc[\"score\"].round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "uuid": "7d5078fb-a218-4c6f-8e22-892004da9bb3"
   },
   "outputs": [],
   "source": [
    "lc_data = df_lc[[\"sentence1\", \"sentence2\", \"score\"]].values.tolist()\n",
    "df_finetune = df_finetune.append(lc_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "uuid": "bb97c8ff-6feb-41cf-ac20-7bb34129b0d1"
   },
   "outputs": [],
   "source": [
    "df_finetune = df_finetune.rename({\n",
    "    0: \"reference\",\n",
    "    1: \"candidate\",\n",
    "    2: \"score\"\n",
    "}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "uuid": "234048b1-8ce9-4327-8f14-21f67f368947"
   },
   "outputs": [],
   "source": [
    "df_finetune = df_finetune.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "uuid": "c86b7484-8d8c-4181-ab3f-09ddddaa604e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_dev = train_test_split(df_finetune, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "uuid": "d747424e-4d06-42e0-b1b7-ebcdec5882d3"
   },
   "outputs": [],
   "source": [
    "df_train.to_json(\"./data_generated/finetune_train.jsonl\", orient='records', force_ascii=False, lines=True)\n",
    "df_dev.to_json(\"./data_generated/finetune_dev.jsonl\", orient='records', force_ascii=False, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "bc444121-fddb-4e88-8b2b-9a9fafce3e7e"
   },
   "source": [
    "# Blend generated samples and finetune samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "uuid": "8bbdd063-4f9d-4d02-98e4-f8d818cf17c3"
   },
   "outputs": [],
   "source": [
    "columns = [\"reference\", \"candidate\", \"score\"]\n",
    "df = pd.concat([dataset[columns], df_finetune[columns]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "uuid": "faa009ae-79e5-43a3-99bc-743076d28bd9"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_json(\"./data_generated/dataset.csv\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "uuid": "46473470-fd1a-4d2b-ae39-38ccbebd70d7"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"./data_generated/finetune_train.jsonl\", lines=True)\n",
    "df3 = pd.read_json(\"./data_generated/finetune_dev.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "uuid": "0163b1d8-8ef5-4d8a-9c00-3ff1328d7abf"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1.sample(int(1e6)), df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "uuid": "ab9c5e4a-4b31-4d74-b2dc-d4c0ce30b570"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_dev = train_test_split(df, test_size=0.01)\n",
    "df_train.to_json(\"./data_generated/rating_train.jsonl\", orient='records', force_ascii=False, lines=True)\n",
    "df_dev.to_json(\"./data_generated/rating_dev.jsonl\", orient='records', force_ascii=False, lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
